#summary Building simple predictive models using fingerprints
#labels Tutorial,ML

= Introduction =

A simple tutorial on building predictive models using the RDKit's machine learning code and molecular descriptors.

= Constructing the dataset =

The learning code expects a list of `[label, descriptor1, descriptor2,..., descriptori, activity]` lists, so let's build this using a literature dataset:
{{{
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.ML.Descriptors import MoleculeDescriptors
import cPickle

ms = [x for x in Chem.SDMolSupplier('hERG_inhibition_dataset.sdf') if x is not None]

nms=[x[0] for x in Descriptors._descList]
# "MolecularFormula" shows up in the list of descriptors, but it's
# not numeric, so remove it from the list:
nms.remove('MolecularFormula')

# a descriptor calculator makes it easy to generate sets of descriptors:
calc = MoleculeDescriptors.MolecularDescriptorCalculator(nms)

# generate the descriptors (can take a while):
descrs = [calc.CalcDescriptors(x) for x in ms]
ndescrs = len(calc.GetDescriptorNames())

pts=[]
for i,m in enumerate(ms):
    if m.GetProp('ACTIVITY_CLASS')=='active':
        act=1
    else:
        act=0
    pts.append([m.GetProp('CompoundName')]+list(descrs[i])+[act])

# now write to disk so we don't have to ever repeat the calculation:
cPickle.dump(pts,file('descrs.pkl','wb+'))
}}}


= Building a single decision tree =
{{{
from rdkit.ML.DecTree.BuildQuantTree import BuildQuantTree,QuantTreeBoot

# number of quantization bounds to automatically find for each descriptor:
boundsPerVar = [0]+[1]*ndescrs+[0]
# number of possible values of each descriptor:
nPossible = [0]+[2]*ndescrs+[2]
# list of descriptors to use:
attrs = range(1,ndescrs+1)

# build the tree:
t = QuantTreeBoot(pts,attrs,nPossible,boundsPerVar)

# generate the confusion matrix:
confusionMat=numpy.zeros((2,2),numpy.int)
for pt in pts:
    confusionMat[pt[-1]][t.ClassifyExample(pt)]+=1

print confusionMat
}}}

The `maxDepth` argument above is the maximum number of layers below the root that will be added to the decision tree. It's a useful way to lower the risk of overfitting.

Decision trees require quantized descriptors (i.e. a small number of possible integer values). QuantTrees (built in the example above) can automatically generate appropriate quantization bounds.

= Building a bag of decision trees =

For historic reasons, bagged classifiers in the RDKit are known as "composite models".

{{{
import cPickle
from rdkit.ML.DecTree.BuildQuantTree import BuildQuantTree
from rdkit.ML.Composite.Composite import Composite
from rdkit.ML.DecTree import CrossValidate
from rdkit.ML import ScreenComposite

pts = cPickle.load(file('pts.pkl','rb'))

# build a composite (bag) with 10 trees:
cmp = Composite()
cmp.Grow(pts,attrs=attrs,nPossibleVals=nPossible,nTries=10,
         buildDriver=CrossValidate.CrossValidationDriver,
         treeBuilder=QuantTreeBoot,needsQuantization=False,nQuantBounds=boundsPerVar,maxDepth=3)

# calculate and print error statistics using the out-of-bag approach:
res = ScreenComposite.ShowVoteResults(range(len(pts)),pts,cmp,2,0,errorEstimate=True)
}}}

Here's what that outputs to console:
{{{
	*** Vote Results ***
misclassified: 68/242 (%28.10)	68/242 (%28.10)

average correct confidence:    0.9196
average incorrect confidence:  0.8287

	Results Table:

          63      27      |  60.00
          41     111      |  79.86
     ------- ------- 
       60.58   80.43 
}}}

This helps to understand the contents of the tuple returned by `ShowVoteResults`:
{{{
(174,
 68,
 0,
 0.91963601532567041,
 0.82867647058823546,
 0.0,
 array([[ 63,  41],
       [ 27, 111]]))
}}}

Predictions from the composite model each have a "confidence" associated with them. This is the fraction of trees that agreed on the prediction (so for a binary model prediction confidence ranges from 0.5 to 1.0). By ignoring predictions with low confidence, the accuracy of the model can sometimes be improved:

{{{
>>> res = ScreenComposite.ShowVoteResults(range(len(pts)),pts,cmp,2,0.7,errorEstimate=True)

	*** Vote Results ***
misclassified: 43/242 (%17.77)	43/189 (%22.75)
skipped: 53/242 (% 21.90)

average correct confidence:    0.9828
average incorrect confidence:  0.9702

	Results Table:

          44       9      |  55.70
          34     102      |  91.07
     ------- ------- 
       56.41   91.89 
}}}

There is some improvement in accuracy at the expense of not classifying 22% of the data.

Composite models can be also be pickled to disk and then reloaded to classify new points:
{{{
}}}
Each result is a `(prediction,confidence)` tuple.